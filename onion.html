<!DOCTYPE html>
<html lang="en">

<head>

    <title>Luke</title>
    <link rel="icon" type="image/x-icon" href="images/favicon-mirrored.png">

    <!-- Required meta tags for Bootstrap -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link 
        rel="stylesheet"
        href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
        integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm"
        crossorigin="anonymous"
    >

    <!-- Pygment's code highlight -->
    <link rel="stylesheet" type="text/css" href="css/monokai.css">

    <!-- Luke's extra highlights -->
    <link rel="stylesheet" type="text/css" href="css/lukes_extra-dark.css">

    <!-- MathJax boilerplate -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        svg: {
          fontCache: 'global'
        }
      };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>

</head>

<body>

    <!-- Bootstrap needs to be in container to work properly -->
    <div class="container">

        <!-- Top Navigation Bar -->
        <nav class="navbar navbar-expand-md bg-dark navbar-dark">
            <!-- Brand -->
            <a class="navbar-brand" href="https://lukefeilberg.github.io">Luke Feilberg ðŸ¤ </a>
            <!-- Toggler/collapsibe Button -->
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#collapsibleNavbar">
              <span class="navbar-toggler-icon"></span>
            </button>
            <!-- Navbar links -->
            <div class="collapse navbar-collapse" id="collapsibleNavbar">
              <ul class="navbar-nav">
                <li class="nav-item">
                  <a class="nav-link" href="https://lukefeilberg.github.io">Home</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="https://lukefeilberg.github.io/projects.html">Projects</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="https://lukefeilberg.github.io/blog.html">Blog</a>
                </li> 
              </ul>
            </div> 
          </nav>
          
        <!-- Jinja fills in the body below -->

        <h1>The Onion Classifier ðŸ§…</h1>
<p><a href="https://www.theonion.com">The Onion</a> is Americas's Finest News Source -- a fantastic site of satirical news headlines. However, non-satirical news outlets seem to often compete with The Onion often blurring the lines on who is actually more absurd. Thus, a subreddit called <a href="https://www.reddit.com/r/nottheonion">r/NotTheOnion</a> was born to collect headlines from supposedly real news stories that could be mistaken for Onion headlines. The subreddit is quite popular with about 22 million members and chalk full of headlines that really can be tough to distinguish from satire.</p>
<p>So, this is supposedly a difficult classification problem for humans which begs the question -- <em>Can a machine learning model do better?</em> ðŸ˜‰</p>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
</code></pre></div>

<h2>Part 1: Getting Data ðŸ“œ</h2>
<p>Data is extracted from Reddit using the Pushshift API as documented here: <a href="https://github.com/pushshift/api">github.com/pushshift/api</a></p>
<h3>Getting dates to extract data between</h3>
<p>The Pushshift API only returns at most 1000 posts with each request, so I create a list of dates to pull ~1000 posts between these dates.</p>
<div class="codehilite"><pre><span></span><code><span class="n">dates_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Making list of dates; Each January 1st and June 1st from 2015 until January 1st 2020</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">21</span><span class="p">):</span>
    <span class="n">dates_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;01/01/20&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
    <span class="n">dates_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;01/06/20&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>

<span class="c1"># Popping June 2020 since it hasn&#39;t happened yet</span>
<span class="n">dates_list</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
<span class="n">dates_list</span>
</code></pre></div>

<div class="outputcode">[
  '01/01/2010',
  '01/06/2010',
  '01/01/2011',
  '01/06/2011',
  '01/01/2012',
  '01/06/2012',
  '01/01/2013',
  '01/06/2013',
  '01/01/2014',
  '01/06/2014',
  '01/01/2015',
  '01/06/2015',
  '01/01/2016',
  '01/06/2016',
  '01/01/2017',
  '01/06/2017',
  '01/01/2018',
  '01/06/2018',
  '01/01/2019',
  '01/06/2019',
  '01/01/2020'
]</div>

<h3>Converting dates to Unix timestamp</h3>
<p>Returns Unix timestamp that Pushshift API requires for dates</p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">getTimeStamp</span><span class="p">(</span><span class="n">date_input</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">time</span><span class="o">.</span><span class="n">mktime</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">strptime</span><span class="p">(</span><span class="n">date_input</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">%d</span><span class="s2">/%m/%Y&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">timetuple</span><span class="p">())</span>

<span class="n">dates</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">getTimeStamp</span><span class="p">(</span><span class="n">date</span><span class="p">))</span> <span class="k">for</span> <span class="n">date</span> <span class="ow">in</span> <span class="n">dates_list</span><span class="p">]</span>
<span class="n">dates</span>
</code></pre></div>

<div class="outputcode">[
  1262332800,
  1275375600,
  1293868800,
  1306911600,
  1325404800,
  1338534000,
  1357027200,
  1370070000,
  1388563200,
  1401606000,
  1420099200,
  1433142000,
  1451635200,
  1464764400,
  1483257600,
  1496300400,
  1514793600,
  1527836400,
  1546329600,
  1559372400,
  1577865600
]</div>

<h3>Getting Pushshift data</h3>
<p>Returns the top 1000 posts in the given subreddit between the given times.<br>
Code modified from the following article:<br>
<a href="https://medium.com/@RareLoot/using-pushshifts-api-to-extract-reddit-submissions-fb517b286563">medium.com/@RareLoot/using-pushshifts-api-to-extract-reddit-submissions-fb517b286563</a></p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">getPushshiftData</span><span class="p">(</span><span class="n">after</span><span class="p">,</span> <span class="n">before</span><span class="p">,</span> <span class="n">sub</span><span class="p">):</span>
    <span class="n">url</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;https://api.pushshift.io/reddit/search/submission/?size=1000&amp;after=&#39;</span><span class="o">+</span>
           <span class="nb">str</span><span class="p">(</span><span class="n">after</span><span class="p">)</span><span class="o">+</span><span class="s1">&#39;&amp;before=&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">before</span><span class="p">)</span><span class="o">+</span><span class="s1">&#39;&amp;subreddit=&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">sub</span><span class="p">)</span><span class="o">+</span><span class="s1">&#39;&amp;sort_type=score&#39;</span><span class="o">+</span><span class="s1">&#39;&amp;sort=desc&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
    <span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
</code></pre></div>

<h3>Getting all the titles between the dates chosen earlier</h3>
<p>Here I loop through all the dates above and get the top ~1000 posts in the chosen subreddit.   </p>
<p>I end up with 9065 Onion headlines and 15432 "fake"-Onion headlines from r/NotThenOnion.<br>
I then keep the first 9000 and first 15000 for easier batching. </p>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">getTitles</span><span class="p">(</span><span class="n">subreddit</span><span class="p">):</span>
    <span class="n">titles_new</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">titles</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dates</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="c1"># Setting up dates</span>
        <span class="n">after</span>  <span class="o">=</span> <span class="n">dates</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">before</span> <span class="o">=</span> <span class="n">dates</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># Getting subreddit data between the dates after and before from r/NotTheOnion</span>
        <span class="n">raw_json</span> <span class="o">=</span> <span class="n">getPushshiftData</span><span class="p">(</span><span class="n">after</span><span class="p">,</span><span class="n">before</span><span class="p">,</span><span class="n">subreddit</span><span class="p">)</span>

        <span class="c1"># Extracting just the title</span>
        <span class="n">titles_new</span> <span class="o">=</span> <span class="p">[</span><span class="n">post</span><span class="p">[</span><span class="s1">&#39;title&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">post</span> <span class="ow">in</span> <span class="n">raw_json</span><span class="p">]</span>

        <span class="c1"># Appending new data on</span>
        <span class="n">titles</span> <span class="o">=</span> <span class="n">titles</span> <span class="o">+</span> <span class="n">titles_new</span>

    <span class="c1"># A few posts were extracted twice, set gets rid of duplicates</span>
    <span class="n">titles</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">titles</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">titles</span>

<span class="n">not_onion</span> <span class="o">=</span> <span class="n">getTitles</span><span class="p">(</span><span class="s1">&#39;nottheonion&#39;</span><span class="p">)</span>
<span class="n">onion</span> <span class="o">=</span> <span class="n">getTitles</span><span class="p">(</span><span class="s1">&#39;theonion&#39;</span><span class="p">)</span>

<span class="n">onion</span> <span class="o">=</span> <span class="n">onion</span><span class="p">[:</span><span class="mi">9000</span><span class="p">]</span>
<span class="n">not_onion</span> <span class="o">=</span> <span class="n">not_onion</span><span class="p">[:</span><span class="mi">15000</span><span class="p">]</span>
</code></pre></div>

<div class="outputcode">https://api.pushshift.io/reddit/search/submission/?size=1000&after=1262332800&before=1275375600&subreddit=nottheonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1275375600&before=1293868800&subreddit=nottheonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1293868800&before=1306911600&subreddit=nottheonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1306911600&before=1325404800&subreddit=nottheonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1325404800&before=1338534000&subreddit=nottheonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1338534000&before=1357027200&subreddit=nottheonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1357027200&before=1370070000&subreddit=nottheonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1370070000&before=1388563200&subreddit=nottheonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1388563200&before=1401606000&subreddit=nottheonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1401606000&before=1420099200&subreddit=nottheonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1420099200&before=1433142000&subreddit=nottheonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1433142000&before=1451635200&subreddit=nottheonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1451635200&before=1464764400&subreddit=nottheonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1464764400&before=1483257600&subreddit=nottheonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1483257600&before=1496300400&subreddit=nottheonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1496300400&before=1514793600&subreddit=nottheonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1514793600&before=1527836400&subreddit=nottheonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1527836400&before=1546329600&subreddit=nottheonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1546329600&before=1559372400&subreddit=nottheonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1559372400&before=1577865600&subreddit=nottheonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1262332800&before=1275375600&subreddit=theonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1275375600&before=1293868800&subreddit=theonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1293868800&before=1306911600&subreddit=theonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1306911600&before=1325404800&subreddit=theonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1325404800&before=1338534000&subreddit=theonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1338534000&before=1357027200&subreddit=theonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1357027200&before=1370070000&subreddit=theonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1370070000&before=1388563200&subreddit=theonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1388563200&before=1401606000&subreddit=theonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1401606000&before=1420099200&subreddit=theonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1420099200&before=1433142000&subreddit=theonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1433142000&before=1451635200&subreddit=theonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1451635200&before=1464764400&subreddit=theonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1464764400&before=1483257600&subreddit=theonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1483257600&before=1496300400&subreddit=theonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1496300400&before=1514793600&subreddit=theonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1514793600&before=1527836400&subreddit=theonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1527836400&before=1546329600&subreddit=theonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1546329600&before=1559372400&subreddit=theonion&sort_type=score&sort=desc
https://api.pushshift.io/reddit/search/submission/?size=1000&after=1559372400&before=1577865600&subreddit=theonion&sort_type=score&sort=desc
</div>

<h3>Converting to pandas dataframe</h3>
<p>Labeling Onion headlines as 1, and r/NotTheOnion headlines as 0.</p>
<div class="codehilite"><pre><span></span><code><span class="n">df1</span><span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;text&#39;</span><span class="p">:</span><span class="n">onion</span><span class="p">})</span>
<span class="n">df1</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

<span class="n">df2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;text&#39;</span><span class="p">:</span><span class="n">not_onion</span><span class="p">})</span>
<span class="n">df2</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># Combining both datasets</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df1</span><span class="p">,</span><span class="n">df2</span><span class="p">])</span>

<span class="c1"># Shuffling the dataset</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Converting all text to lowercase, fixing ampersands and getting rid</span>
<span class="c1"># of dashes and apostrophes as they can mess up the dictionary</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;&amp;amp;&#39;</span><span class="p">,</span> <span class="s1">&#39;and&#39;</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;-&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[^\s\w]&#39;</span><span class="p">,</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="c1"># Saving the dataframe to a csv file</span>
<span class="n">df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s1">&#39;OnionOrNot.csv&#39;</span><span class="p">)</span>

<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>egypt funeral turns happy after dead man awakes</td>
      <td>0</td>
    </tr>
    <tr>
      <td>1</td>
      <td>michael jordan attacks softness lack of compet...</td>
      <td>1</td>
    </tr>
    <tr>
      <td>2</td>
      <td>feds bust massive child pornography corporatio...</td>
      <td>1</td>
    </tr>
    <tr>
      <td>3</td>
      <td>joker arrested for flashing his penis   but it...</td>
      <td>0</td>
    </tr>
    <tr>
      <td>4</td>
      <td>mexican lawmaker wants to fine people 1600 for...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<h3>Reading in dataframe</h3>
<p>Running this when I return to the project so I don't have to use the Pushshift API etc. again.</p>
<div class="codehilite"><pre><span></span><code><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;OnionOrNot.csv&#39;</span><span class="p">,</span> <span class="n">index_col</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>egypt funeral turns happy after dead man awakes</td>
      <td>0</td>
    </tr>
    <tr>
      <td>1</td>
      <td>michael jordan attacks softness lack of compet...</td>
      <td>1</td>
    </tr>
    <tr>
      <td>2</td>
      <td>feds bust massive child pornography corporatio...</td>
      <td>1</td>
    </tr>
    <tr>
      <td>3</td>
      <td>joker arrested for flashing his penis   but it...</td>
      <td>0</td>
    </tr>
    <tr>
      <td>4</td>
      <td>mexican lawmaker wants to fine people 1600 for...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>

<h2>Part 2: Encoding Words as Numbers ðŸ”¢</h2>
<h3>Getting all the words in the training data</h3>
<div class="codehilite"><pre><span></span><code><span class="n">vocab_set</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
<span class="n">sentence_lengths</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)):</span>
    <span class="c1"># Updates adds all items to the set, re.split splits the text into words</span>
    <span class="n">sentence_words</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;\s&#39;</span><span class="p">,</span><span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;text&#39;</span><span class="p">])</span>
    <span class="n">vocab_set</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">sentence_words</span><span class="p">)</span>
    <span class="n">sentence_lengths</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sentence_words</span><span class="p">))</span>
</code></pre></div>

<h3>Converting the words to a dictionary</h3>
<p>This way we can map the words in the dataframe to lists of numbers</p>
<div class="codehilite"><pre><span></span><code><span class="n">vocab_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">vocab_set</span><span class="p">)</span>
<span class="n">vocab_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">vocab_list</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab_list</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)}</span>
</code></pre></div>

<h3>Creating column of the words mapped to numbers</h3>
<div class="codehilite"><pre><span></span><code><span class="n">max_length</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">sentence_lengths</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">toNumbers</span><span class="p">(</span><span class="n">row</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">findall</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;([\w]+)&#39;</span><span class="p">,</span> <span class="n">row</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span>
    <span class="n">nums</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">vocab_dict</span><span class="p">[</span><span class="n">words</span><span class="p">[</span><span class="n">j</span><span class="p">]]</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">))])</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">nums</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_length</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">nums</span><span class="p">)),</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">)</span>
</code></pre></div>

<p><br></p>
<div class="codehilite"><pre><span></span><code><span class="n">nums</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">row</span><span class="p">:</span> <span class="n">toNumbers</span><span class="p">(</span><span class="n">row</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> 
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;nums&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">nums</span>

<span class="n">df</span><span class="p">[</span><span class="s1">&#39;nums&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div>

<div class="outputcode">    0    [10101, 15701, 24365, 6689, 22221, 4330, 4928,...
    1    [6556, 7335, 1523, 21250, 6690, 23567, 18468, ...
    2    [20493, 17894, 4253, 9925, 21346, 24068, 7515,...
    3    [18219, 15505, 9902, 24892, 16634, 10504, 810,...
    4    [16068, 3826, 16392, 14837, 1613, 5793, 15082,...
    Name: nums, dtype: object
</div>

<h3>Converting to Numpy arrays</h3>
<div class="codehilite"><pre><span></span><code><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;nums&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="n">features</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">labels</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div>

<div class="outputcode">((24000, 64), (24000,))
</div>

<h2>Part 3: Building and Fitting the Model ðŸ”¬</h2>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_compiled_model</span><span class="p">():</span>
    <span class="n">embedding_dim</span><span class="o">=</span><span class="mi">16</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab_set</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Bidirectional</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span>  <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">)),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Bidirectional</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">32</span><span class="p">)),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">)</span>
    <span class="p">])</span>


    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span>
                <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;binary_crossentropy&#39;</span><span class="p">,</span>
                <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">model</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">get_compiled_model</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span>
</code></pre></div>

<div class="outputcode">Train on 19200 samples, validate on 4800 samples
    Epoch 1/5
    19200/19200 - 173s - loss: 0.4103 - accuracy: 0.8130 - val_loss: 0.3051 - val_accuracy: 0.8715
    Epoch 2/5
    19200/19200 - 152s - loss: 0.1713 - accuracy: 0.9377 - val_loss: 0.3448 - val_accuracy: 0.8652
    Epoch 3/5
    19200/19200 - 116s - loss: 0.0685 - accuracy: 0.9778 - val_loss: 0.4366 - val_accuracy: 0.8554
    Epoch 4/5
    19200/19200 - 121s - loss: 0.0348 - accuracy: 0.9891 - val_loss: 0.6293 - val_accuracy: 0.8344
    Epoch 5/5
    19200/19200 - 127s - loss: 0.0242 - accuracy: 0.9923 - val_loss: 0.6790 - val_accuracy: 0.8442
</div>

<h3>Tuning parameters, modifying model, etc.</h3>
<h4>Trial 1: ~85% validation accuracy around epoch 5</h4>
<div class="codehilite"><pre><span></span><code><span class="n">embedding_dim</span><span class="o">=</span><span class="mi">16</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
  <span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab_set</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">),</span>
  <span class="n">layers</span><span class="o">.</span><span class="n">GlobalAveragePooling1D</span><span class="p">(),</span>
  <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
  <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div>

<h4>Trial 2: ~85% validation accuracy around epoch 5</h4>
<p>No discernable change in accuracy tuning embedding_dim </p>
<div class="codehilite"><pre><span></span><code><span class="n">embedding_dim</span><span class="o">=</span><span class="mi">32</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
  <span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab_set</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">),</span>
  <span class="n">layers</span><span class="o">.</span><span class="n">GlobalAveragePooling1D</span><span class="p">(),</span>
  <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
  <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div>

<h4>Trial 3: ~87% validation accuracy on epoch 1</h4>
<p>Starts overfitting after epoch 1</p>
<div class="codehilite"><pre><span></span><code><span class="n">embedding_dim</span><span class="o">=</span><span class="mi">16</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab_set</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Bidirectional</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">64</span><span class="p">)),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div>

<h4>Trial 4: ~87% validation accuracy on epoch 1</h4>
<p>Again starts overfitting after epoch 1</p>
<div class="codehilite"><pre><span></span><code><span class="n">embedding_dim</span><span class="o">=</span><span class="mi">16</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab_set</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Bidirectional</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span>  <span class="n">return_sequences</span><span class="o">=</span><span class="kc">True</span><span class="p">)),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Bidirectional</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">32</span><span class="p">)),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div>

<h2>Part 4: Conclusion ðŸ¤–</h2>
<p>So, playing around with some simple models and parameters it appears we can get about 87% validation accuracy. I haven't done a true deep dive but after shuffling the "real" vs "fake" headlines and investigating myself I seem to be about that accurate, perhaps a bit more!</p>
<p>You can download the CSV of all the data in my <a href="https://github.com/lukefeilberg/onion">GitHub repo</a> for the project (or of course follow along above to get the data yourself!). If you check it out let me know if you try any models and beat my "benchmarks" above ðŸ˜œ. You can always reach me by the email in the footer!</p>

        <!-- End of Jinja filled in body -->


        <!-- Footer -->
        <footer class="footer">

            <p>ðŸ“«: lukefeilberg@gmail.com</p>
        
        </footer>
        <!-- Footer -->

    </div>

    <!-- Javascript libraries for Bootstrap -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

</body>

</html>