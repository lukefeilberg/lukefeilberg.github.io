<!DOCTYPE html>
<html lang="en">

<head>

    <title>Luke</title>
    <link rel="icon" type="image/png" href="images/favicon-mirrored.png">

    <!-- Required meta tags for Bootstrap -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link 
        rel="stylesheet"
        href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
        integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm"
        crossorigin="anonymous"
    >

    <!-- Pygment's code highlight -->
    <link rel="stylesheet" type="text/css" href="css/monokai.css">

    <!-- Luke's extra highlights -->
    <link rel="stylesheet" type="text/css" href="css/lukes_extra-dark.css">

    <!-- MathJax boilerplate -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        svg: {
          fontCache: 'global'
        }
      };
    </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>

    <!-- Counter.dev -->
    <script src="https://cdn.counter.dev/script.js" data-id="0aeed35e-dde5-4063-8903-dbb24d498964" data-utcoffset="-7"></script>

</head>

<body>

    <!-- Bootstrap needs to be in container to work properly -->
    <div class="container">

        <!-- Top Navigation Bar -->
        <nav class="navbar navbar-expand-md bg-dark navbar-dark">
            <!-- Brand -->
            <a class="navbar-brand" href="https://lukefeilberg.github.io">Luke Feilberg ü§†</a>
            <!-- Toggler/collapsibe Button -->
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#collapsibleNavbar">
              <span class="navbar-toggler-icon"></span>
            </button>
            <!-- Navbar links -->
            <div class="collapse navbar-collapse" id="collapsibleNavbar">
              <ul class="navbar-nav">
                <li class="nav-item">
                  <a class="nav-link" href="https://lukefeilberg.github.io">Home</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="https://lukefeilberg.github.io/projects.html">Projects</a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="https://lukefeilberg.github.io/blog.html">Blog</a>
                </li> 
              </ul>
            </div> 
          </nav>
          
        <!-- Jinja fills in the body below -->

        <h1>Blackjack Policy Iteration üÉè</h1>
<p>After completing CS 188 (Intro to Artificial Intelligence) at my school I felt inspired to try out some of the techniques I had learned. It occurred to me while taking the class that blackjack could be modeled as a Markov Decision Process (MDP), and thus I could learn the optimal policy (or strategy) by performing Policy Iteration on it. For more general conceptual info see <a href="http://ai.berkeley.edu/home.html" target="_blank">ai.berkeley.edu</a>.</p>
<p>All the code for this project can be found at <a href="https://github.com/lukefeilberg/blackjack" target="_blank">github.com/lukefeilberg/blackjack</a>.</p>
<h2>Modeling the States ‚ô†</h2>
<p>For this project I‚Äôve first denoted states as tuples <code>(num_aces, sum)</code>, where <code>num_aces</code> denotes the number aces drawn so far, and <code>sum</code> denotes the running sum of the values of cards drawn so far minus the ace values since they can either be one or eleven (hence why <code>num_aces</code> is stored separately). These tuples are a bit clunky to work with so I then number them from 0 to 216 for later simplicity, shown in the next section!</p>
<h2>Our Beautiful State Space üó∫</h2>
<p>I'll denote our set of possible states with the symbol $\mathcal{S}$. We thus have $\mathcal{S} = [0,216]$, and each integer maps to the more interpretable tuple <code>(num_aces, sum)</code> as shown in the table below:</p>
<p style="margin-left: 40px">
<font face="consolas">

0:&ensp;&nbsp;   (0,0),  1:&ensp;&nbsp;  (0,2),  2:&ensp;&nbsp;  (0,3), ..., 29:&ensp;  (0,30), 30:&ensp;  (0,31)<br>
31:&ensp;  (1,0), 32:&ensp;  (1,2), 33:&ensp;  (1,3), ..., 60:&ensp;  (1,30), 61:&ensp;  (1,31) <br>
62:&ensp;  (2,0), 63:&ensp;  (2,2), 64:&ensp;  (2,3), ..., 91:&ensp;  (2,30), 92:&ensp;  (2,31) <br>
... <br>
186: (6,0), 187: (6,2), 188: (6,3), ..., 215: (6,30), 216: (6,31) </p>
<p></font>
Note: <code>(n,1)</code> is not included for any <code>n</code> since drawing a 1 would actually be drawing an ace and is thus captured by <code>(n+1, 0)</code> instead. Also note that ending at 6 aces was arbitrary but pulling 7 or more aces seemed sufficiently unlikely for me to care about how to play in that situation. (Note too that casinos often play with seven or eight decks, thus why we have the possibility of more than 4 aces in the first place). Lastly having the <code>sum</code> run to 31 was an arbitrary choice in order to give a much more legible transition function.</p>
<h2>Actions üõπ</h2>
<p>In blackjack you have two possible actions:</p>
<ol>
<li>$\text{draw}$: draw another card and add to your sum.</li>
<li>$\text{end}$: end the game with your current total.</li>
</ol>
<p>We'll denote the set of all possible actions as $\mathcal{A}$, thus we have $\mathcal{A}=\{\text{draw}, \text{stop}\}$.</p>
<h2>Transitioning States üîÄ</h2>
<p>First, I‚Äôll make the assumption that when we draw cards we'll replace them back in the deck -- this way our strategy doesn‚Äôt have to depend on the specific cards drawn/left (plus casinos often play with around eight decks anyways). </p>
<p>Thus there is a $\frac{1}{13}$ chance when you draw you‚Äôll pull an ace. Since there are 31 columns $T(s, \text{draw}, s+31) = \frac{1}{13}$ (recall that adding 31 takes you down exactly one row as should be clear by looking at the sample of states provided above). Similarly there is a $\frac{1}{13}$ chance of pulling a 2 and increasing your sum by 2, a $\frac{1}{13}$ chance of pulling a 3 and increasing your sum by 3, and so forth giving: $T(s, \text{draw}, s+k) = \frac{1}{13}, \;\, \forall k \in [2,9]$ . Lastly, since 10, jack, queen, and kings all have a value of 10, those four ranks yield: $T(s, \text{draw}, s+10) = \frac{4}{13}$. Any other transition is impossible and will return a value of zero. Since I‚Äôm going to need these values very often I‚Äôm going to compute them once in a matrix called <code>draw_matrix</code> during initialization. </p>
<h2>Defining Rewards ü•á</h2>
<p>You only receive a reward in blackjack for exiting the game with a sum less than or equal to 21. Thus if your action is stop you receive your sum plus the optimal value of your aces, or zero if this is greater than 21. For example:</p>
<ul>
<li>$R\left((0,10), \text{stop}, \text{game over} \right) = 10$ <ul>
<li><em>No aces, a sum of 10 gives us a value of 10.</em></li>
</ul>
</li>
<li>$R\left((1,10), \text{stop}, \text{game over} \right) = 21$ <ul>
<li><em>One ace, and a sum of 10 yields 21 since that is the best case scenario and beats treating the ace as a one.</em></li>
</ul>
</li>
<li>$R\left((2,10), \text{stop}, \text{game over} \right) = 12$ <ul>
<li><em>With two aces and a sum of 10 we only don't bust if we treat both aces as ones -- thus yielding 12.</em></li>
</ul>
</li>
<li>$R\left((2,20), \text{stop}, \text{game over} \right) = 0$ <ul>
<li><em>With two aces and a sum of 20 there is no hope, we busted! Zero reward.</em></li>
</ul>
</li>
</ul>
<h2>Policy Iteration ü§π‚Äç‚ôÇÔ∏è</h2>
<p>Policy Iteration begins by first defining an arbitrary initial policy, any policy will work but the closer to the optimal policy the quicker the algorithm converges. I‚Äôll define the initial policy as ‚Äúdraw‚Äù no matter what to make initialization easy and to hammer home the point that we‚Äôll find the optimal policy despite a terrible initial policy! Now we‚Äôll repeat the following two steps until convergence:</p>
<ol>
<li><strong>Policy Evaluation</strong>, and </li>
<li><strong>Policy Improvement</strong>. </li>
</ol>
<h2>Policy Evaluation üîé</h2>
<p>In the Policy Evaluation step we evaluate the expected value of our score for each state. This will be a system of $|\mathcal{S}|$ equations given by the (lightly modified) Bellman Equation:
$$ 
V^{\pi}(s) = \sum_{s' \in \mathcal{S}} T(s, \pi(s), s') \cdot \left( R \left(s,\pi(s), s' \right) + \gamma V^{\pi}(s') \right) 
$$
Since there's no discount in blackjack (scores remain the same no matter the number of timesteps taken to obtain sum/score) we can set $\gamma = 1$ yielding:</p>
<p>$$ 
V^{\pi}(s) = \sum_{s'\in \mathcal{S}} T(s, \pi(s), s') \cdot \left( R \left(s,\pi(s), s' \right) + V^{\pi}(s') \right) 
$$
Now if the policy of a state is ‚Äúdraw‚Äù we obtain the following equation:
$$ 
V^{\pi}(s) = \sum_{s'\in \mathcal{S}} T(s, \text{draw}, s') \cdot \left( R \left(s,\text{draw}, s' \right) + V^{\pi}(s') \right) 
$$
The reward for drawing is always zero, because we only get rewarded for stopping before 21! This gives us $R \left(s,\text{draw}, s' \right) = 0,\;\; \forall s, s' \in S$:</p>
<p>$$
\begin{align}
V^{\pi}(s) &amp;= \sum_{s'\in \mathcal{S}} T(s, \text{draw}, s') \cdot \left(  V^{\pi}(s') \right)  \newline
%
V^{\pi}(s) &amp;= \frac{1}{13}\cdot V^{\pi}(s+31) 
   + \left[ 
      \sum_{k=2}^{9} \frac{1}{13} \cdot V^{\pi}(s+k) 
     \right]
   + \frac{4}{13} \cdot V^{\pi} (s+10)  \newline
%
\implies
%
0 &amp;= -V^{\pi}(s) + 
   \frac{1}{13}\cdot V^{\pi}(s+31) 
   + \left[ 
      \sum_{k=2}^{9} \frac{1}{13} \cdot V^{\pi}(s+k) 
     \right]
   + \frac{4}{13} \cdot V^{\pi} (s+10)<br>
\end{align}
$$
Otherwise, if the policy of a state is ‚Äústop‚Äù, there is no possible states to transition to (we'll denote this as an extra state just called $\text{game over}$) yielding simply:
$$
\begin{align}
%
  V^{\pi}(s) &amp;= R(s, \text{stop}, \text{game over})  \newline
%
  \implies
  -R(s, \text{stop}, \text{game over}) &amp;= -V^{\pi}(s)
%<br>
\end{align}
$$
Now we have our system of equations in matrix form and can use <code>numpy</code>‚Äôs built in system of equation solver to compute the value of each state under our given policy.</p>
<h2>Policy Improvement üìà</h2>
<p>Now that we have the value of each state under our given policy, let‚Äôs go ahead and improve our policy! Using our computed values of each state from above we will create a new policy that selects the action that gives the maximum score, according to the equation:
$$\pi_{i+1}(s) = \text{argmax}_{a\in\mathcal{A}} 
  \sum_{s'\in\mathcal{S}} T \left( s, a, s' \right) 
  \cdot \left[ R\left( s, a, s' \right) + \gamma V^{\pi}(s') \right]
$$
Now notice that if our action is <code>‚Äústop‚Äù</code> we simply return the reward of stopping at that state. Therefore <code>‚Äústop‚Äù</code> does not depend on our computed values of states. This means we only have to compute this value once and can then store it! We‚Äôll do that in an array called <code>stop_value</code>. The values of <code>‚Äúdraw‚Äù</code> in each state can be computed by multiplying our transition matrix by our array of computed values, let‚Äôs save this as an array called <code>draw_value</code>. Now we can define our new policy to be <code>‚Äúdraw‚Äù</code> everywhere except the indices where <code>stop_value</code> is greater than <code>draw_value</code>. 
If our new policy is equivalent to our old policy then the algorithm has converged and we have found the optimal policy! Otherwise we return to the policy evaluation step and repeat all the above until our policy converges.</p>
<h2>Our Optimal Policy üóª</h2>
<p>In our case we converge to the following policy (recall the formatting is (number of aces, running sum excluding aces):</p>
<div style="border:1px solid black;overflow-y:hidden;overflow-x:scroll;margin-left: 40px;white-space:nowrap">
<p style="width:340%;">
<font face="consolas">
(0, 0): draw | (0, 2): draw | (0, 3): draw | (0, 4): draw | (0, 5): draw | (0, 6): draw | (0, 7): draw | (0, 8): draw | (0, 9): draw | (0, 10): draw | (0, 11): draw | (0, 12): draw | (0, 13): stop | (0, 14): stop | (0, 15): stop | (0, 16): stop | (0, 17): stop | (0, 18): stop | (0, 19): stop | (0, 20): stop | (0, 21): stop<br>

(1, 0): draw | (1, 2): draw | (1, 3): draw | (1, 4): draw | (1, 5): draw | (1, 6): draw | (1, 7): draw | (1, 8): stop | (1, 9): stop | (1, 10): stop | (1, 11): draw | (1, 12): stop | (1, 13): stop | (1, 14): stop | (1, 15): stop | (1, 16): stop | (1, 17): stop | (1, 18): stop | (1, 19): stop | (1, 20): stop | (1, 21): stop <br>

(2, 0): draw | (2, 2): draw | (2, 3): draw | (2, 4): draw | (2, 5): draw | (2, 6): draw | (2, 7): stop | (2, 8): stop | (2, 9): stop | (2, 10): draw | (2, 11): stop | (2, 12): stop | (2, 13): stop | (2, 14): stop | (2, 15): stop | (2, 16): stop | (2, 17): stop | (2, 18): stop | (2, 19): stop | (2, 20): stop | (2, 21): stop <br>

(3, 0): draw | (3, 2): draw | (3, 3): draw | (3, 4): draw | (3, 5): draw | (3, 6): stop | (3, 7): stop | (3, 8): stop | (3, 9): draw | (3, 10): stop | (3, 11): stop | (3, 12): stop | (3, 13): stop | (3, 14): stop | (3, 15): stop | (3, 16): stop | (3, 17): stop | (3, 18): stop | (3, 19): stop | (3, 20): stop | (3, 21): stop <br>

(4, 0): draw | (4, 2): draw | (4, 3): draw | (4, 4): draw | (4, 5): stop | (4, 6): stop | (4, 7): stop | (4, 8): draw | (4, 9): stop | (4, 10): stop | (4, 11): stop | (4, 12): stop | (4, 13): stop | (4, 14): stop | (4, 15): stop | (4, 16): stop | (4, 17): stop | (4, 18): stop | (4, 19): stop | (4, 20): stop | (4, 21): stop <br>

(5, 0): draw | (5, 2): draw | (5, 3): draw | (5, 4): stop | (5, 5): stop | (5, 6): stop | (5, 7): draw | (5, 8): stop | (5, 9): stop | (5, 10): stop | (5, 11): stop | (5, 12): stop | (5, 13): stop | (5, 14): stop | (5, 15): stop | (5, 16): stop | (5, 17): stop | (5, 18): stop | (5, 19): stop | (5, 20): stop | (5, 21): stop <br>

(6, 0): draw | (6, 2): stop | (6, 3): stop | (6, 4): stop | (6, 5): stop | (6, 6): stop | (6, 7): stop | (6, 8): stop | (6, 9): stop | (6, 10): stop | (6, 11): stop | (6, 12): stop | (6, 13): stop | (6, 14): stop | (6, 15): stop | (6, 16): stop | (6, 17): stop | (6, 18): stop | (6, 19): stop | (6, 20): stop | (6, 21): stop <br>
</font>
</p>
</div>

<h2>Analysis üî¢</h2>
<p>My computed policy seems fairly conservative, for example stopping at a sum of 13 with no aces. To aid me in investigating I recreated my program in a far more simplified manner letting aces only take on the value of one, thus states can be represented solely by the running sum of the cards drawn. Under this simplified version the optimal policy says to ‚Äústop‚Äù at states 12 and above. However, let‚Äôs consider state 14. We only will bust if we draw an 8, 9, 10, or face card. Thus we have a <code>6/13</code> chance of busting which is less than half. Let‚Äôs thus define a second policy to ‚Äústop‚Äù at states 15 and above. </p>
<p>Now, when I play a hundred thousand games of blackjack under the computed optimal policy I get a score (or average sum) of about 16.02. When I play a hundred thousand games with my second strategy I get a score (average sum) of about 15.01. So our computed policy does have a higher expected score as we hoped. HOWEVER, when I play these two strategies against each other, the second policy performs better! In fact our computed policy wins 37,557 of 100,000 games and the second policy wins 54,209 (with the rest being ties). The lesson here is to be very careful how you define your reward function. I prioritized coming out with the highest score overall, rather than outscoring my component. Another interesting lesson is that a strategy with a higher expected score does not guarantee outperforming a lesser expected score strategy on average. </p>

        <!-- End of Jinja filled in body -->


        <!-- Footer -->
        <footer class="footer">

            <p>üì´: lukefeilberg@gmail.com</p>
        
        </footer>
        <!-- Footer -->

    </div>

    <!-- JavaScript libraries for Bootstrap -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

</body>

</html>